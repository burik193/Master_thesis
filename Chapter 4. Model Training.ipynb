{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb24f84",
   "metadata": {},
   "source": [
    "# Table Of Contents<a class=\"anchor\" id=\"zero-bullet\"></a>:\n",
    "* [Imports](#first-bullet)\n",
    "* [General settings and Global Functions](#second-bullet)\n",
    "* [Import spectrograms](#third-bullet)\n",
    "* [Model Initialization](#fourth-bullet)\n",
    "* [Training](#fifth-bullet)\n",
    "* [Instant Evaluation Logic](#sixth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a76c9",
   "metadata": {},
   "source": [
    "## Imports<a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b661a0b",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a63dd69d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Oleg\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: Could not find module 'C:\\Users\\Oleg\\anaconda3\\Lib\\site-packages\\torchvision\\image.pyd' (or one of its dependencies). Try using the full path with constructor syntax.\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import lr_scheduler\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import seaborn as sns\n",
    "import json\n",
    "\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d75d78",
   "metadata": {},
   "source": [
    "## General settings and Global Functions<a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd34207",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc758c66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22eba05ebb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af8304fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246e8904",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e041075",
   "metadata": {},
   "source": [
    "## Import spectrograms<a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b6a41",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2976696e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "root = r'C:\\~\\imdb_speechbrain_1000_last_names_robust'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dd1e41",
   "metadata": {},
   "source": [
    "### Load specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "657adc4c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spectrograms = torch.load(root + '/mel_spectrograms.pt')\n",
    "labels_encoded = torch.load(root + '/labels_encoded.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e772c658",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000, 1, 80, 100])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spectrograms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71edd039",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100000])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "002af29f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(root + '/labels_dict.json', 'r', encoding='utf-8') as f:\n",
    "    labels = json.loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4f2b378",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_labels = len(labels)\n",
    "len_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e68b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Initialization<a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbc0440",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0183733",
   "metadata": {},
   "source": [
    "Make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "133e6dc3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "spectrograms = TensorDataset(spectrograms, labels_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3faea02a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define the hyperparameters.\n",
    "batch_size = 32\n",
    "num_epochs = 100\n",
    "learning_rate = 0.0001\n",
    "seed = 42  # Replace with your desired random seed.\n",
    "input_shape = (1, 80, 100)\n",
    "len_labels = 10000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59f7e4ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_dataset, val_dataset = random_split(spectrograms, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55f656",
   "metadata": {},
   "source": [
    "Split the data into Train and Test parts (70/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46910808",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define indices of samples to include in validation set\n",
    "val_indices1 = torch.arange(7, len(spectrograms), 10)\n",
    "val_indices2 = torch.arange(8, len(spectrograms), 10)\n",
    "val_indices3 = torch.arange(9, len(spectrograms), 10)\n",
    "\n",
    "concat_indices = torch.cat((val_indices1, val_indices2, val_indices3), dim=0)\n",
    "inverse_indeces = [i for i in range(len(spectrograms)) if i not in concat_indices]\n",
    "\n",
    "# Use the selected indices to create a Subset of the TensorDataset\n",
    "val_dataset = torch.utils.data.Subset(spectrograms, concat_indices)\n",
    "train_dataset = torch.utils.data.Subset(spectrograms, inverse_indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c9499cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  140000\n",
      "Val size:  60000\n"
     ]
    }
   ],
   "source": [
    "# Load the data samples into PyTorch datasets and dataloaders.\n",
    "print(\"Train size: \", len(train_dataset))\n",
    "print(\"Val size: \", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46e12370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset = DataLoader(train_dataset, shuffle=seed, batch_size=batch_size)\n",
    "val_dataset = DataLoader(val_dataset, shuffle=seed, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c8fba",
   "metadata": {},
   "source": [
    "Note: we don't initialize Softmax layer explicitley, since it is already pre-implemented inside of CrossValidation Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f55a0e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# SCNN ver 1 (relu).\n",
    "class simple_CNN_relu(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64000, 2048),\n",
    "            nn.BatchNorm1d(num_features=2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, num_labels),\n",
    "            # nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = F.normalize(x, mean=[0.5], std=[0.5])\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# SCNN ver 1 (sigm).\n",
    "class simple_CNN_sigm(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64000, 2048),\n",
    "            nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(2048, num_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # x = F.normalize(x, mean=[0.5], std=[0.5])\n",
    "        x = self.conv_layers(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# SCNN ver 2 (and 2.4) (sigm or relu).\n",
    "class lstm_CNN_simple(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.output_height = 256\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True, dropout=self.dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(256, 2048),\n",
    "            nn.BatchNorm1d(num_features=2048),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(2048, num_labels),\n",
    "            # nn.Softmax(dim=0)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        # x = F.resize(x, (self.input_height, self.input_width))\n",
    "        # x = F.normalize(x, mean=[0.5], std=[0.5])\n",
    "        # print(x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        x_hor = nn.AdaptiveAvgPool2d((1, 25))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor = x_hor.squeeze().permute(0, 2, 1)\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor , (hn_hor, cn_hor) = self.lstm(x_hor)\n",
    "        \n",
    "        # Uncomment for 2-axis LSTM\n",
    "        # x_ver = nn.AdaptiveAvgPool2d((20, 1))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver = x_ver.squeeze().permute(0, 2, 1)\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver , (hn_ver, cn_ver) = self.lstm(x_ver)\n",
    "        # print('x: ', x.shape)\n",
    "        # print('hn: ', hn.shape)\n",
    "        # x = torch.concat([hn_hor[-1], hn_ver[-1]], dim=1)\n",
    "        # print(x.shape)\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        # print(x.shape)\n",
    "        x = hn_hor[-1]\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# SCNN ver 3 and 4 (ReLU).\n",
    "class lstm_CNN_relu(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.output_height = 256\n",
    "        \n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True, dropout=self.dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 12800),\n",
    "            nn.BatchNorm1d(num_features=12800),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(12800, num_labels),\n",
    "            # nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = F.resize(x, (self.input_height, self.input_width))\n",
    "        # x = F.normalize(x, mean=[0.5], std=[0.5])\n",
    "        # print(x.shape)\n",
    "        x = self.conv_layers(x)\n",
    "        x_hor = nn.AdaptiveAvgPool2d((1, 25))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor = x_hor.squeeze().permute(0, 2, 1)\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor , (hn_hor, cn_hor) = self.lstm(x_hor)\n",
    "        \n",
    "        # # Uncomment for 2-axis LSTM\n",
    "        # x_ver = nn.AdaptiveAvgPool2d((20, 1))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver = x_ver.squeeze().permute(0, 2, 1)\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver , (hn_ver, cn_ver) = self.lstm(x_ver)\n",
    "        # print('x: ', x.shape)\n",
    "        # print('hn: ', hn.shape)\n",
    "        # x = torch.concat([hn_hor[-1], hn_ver[-1]], dim=1)\n",
    "        # print(x.shape)\n",
    "        x = hn_hor[-1]\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# SCNN ver 3 and 4 (sigm).\n",
    "class lstm_CNN_sigm(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "        self.output_height = 256\n",
    "\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=128),\n",
    "            nn.Sigmoid(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(num_features=256),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Dropout(self.dropout),\n",
    "        )\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=256, hidden_size=512, num_layers=2, batch_first=True, bidirectional=True, dropout=self.dropout)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, 12800),\n",
    "            nn.BatchNorm1d(num_features=12800),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12800, num_labels),\n",
    "            # nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layers(x)\n",
    "        x_hor = nn.AdaptiveAvgPool2d((1, 25))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor = x_hor.squeeze().permute(0, 2, 1)\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor , (hn_hor, cn_hor) = self.lstm(x_hor)\n",
    "        x = hn_hor[-1]\n",
    "        \n",
    "        # Uncomment for 2-axis LSTM\n",
    "        # x_ver = nn.AdaptiveAvgPool2d((20, 1))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver = x_ver.squeeze().permute(0, 2, 1)\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        # x_ver , (hn_ver, cn_ver) = self.lstm(x_ver)\n",
    "        # print('x: ', x.shape)\n",
    "        # print('hn: ', hn.shape)\n",
    "        # x = torch.concat([hn_hor[-1], hn_ver[-1]], dim=1)\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "# LSTM only model architecture.\n",
    "class lstm_nn(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dropout = 0.25\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=2, batch_first=True, bidirectional=True, dropout=0.25)\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(512, int(12288/2)),\n",
    "            nn.BatchNorm1d(num_features=int(12288/2)),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(int(12288/2), 12288),\n",
    "            nn.BatchNorm1d(num_features=12288),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(12288, num_labels),\n",
    "            # nn.Softmax(dim=0)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = F.resize(x, (self.input_height, self.input_width))\n",
    "        # x = F.normalize(x, mean=[0.5], std=[0.5])\n",
    "        # print(x.shape)\n",
    "        # x = self.conv_layers(x)\n",
    "        x_hor = nn.AdaptiveAvgPool2d((1, 100))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor = x_hor.squeeze(1).permute(0, 2, 1)\n",
    "        # print('x_hor: ', x_hor.shape)\n",
    "        x_hor , (hn_hor, cn_hor) = self.lstm(x_hor)\n",
    "\n",
    "        x_ver = nn.AdaptiveAvgPool2d((80, 1))(x)  # Replace 'sequence_length' with the desired value\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        x_ver = x_ver.squeeze(3).permute(0, 2, 1)\n",
    "        # print('x_ver: ', x_ver.shape)\n",
    "        x_ver , (hn_ver, cn_ver) = self.lstm(x_ver)\n",
    "        # print('x: ', x.shape)\n",
    "        # print('hn: ', hn.shape)\n",
    "        x = torch.concat([hn_hor[-1], hn_ver[-1]], dim=1)\n",
    "        # print(x.shape)\n",
    "        # x = x.permute(1, 0, 2)\n",
    "        # print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5378e5f",
   "metadata": {},
   "source": [
    "Create optional methods for weight initialization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "059b72c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def weights_init_he(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(m.weight.data, mode='fan_in')\n",
    "        nn.init.zeros_(m.bias.data)\n",
    "\n",
    "def weights_init_xavier(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight.data)\n",
    "        nn.init.zeros_(m.bias.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "62d54e9c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_CNN_sigm(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): Sigmoid()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): Sigmoid()\n",
       "    (11): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=12800, bias=True)\n",
       "    (1): BatchNorm1d(12800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): Sigmoid()\n",
       "    (3): Linear(in_features=12800, out_features=10000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the CNN model and the loss and optimizer functions.\n",
    "model = lstm_CNN_sigm(num_labels=len_labels)\n",
    "\n",
    "# Init weights (optional)\n",
    "#model.apply(weights_init_xavier)\n",
    "\n",
    "# Init loss function (with or without label weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Init Optimization Algorithm\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# define the scheduler\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True,)\n",
    "\n",
    "# Put the model on the CPU or GPU (Cuda)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93952f8e",
   "metadata": {},
   "source": [
    "## Training<a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d647f14",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545fe3fe",
   "metadata": {},
   "source": [
    "Chose the path and save the desired architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27064ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_model = r'models\\complex_CNN_lstm_sigmoid'\n",
    "architecture_num = 5\n",
    "run = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b963215",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open(fr'{path_to_model}\\architecture_{architecture_num}\\architecture.txt', 'w') as f:\n",
    "    print(model, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697240dc",
   "metadata": {},
   "source": [
    "Ensure that the path to the model folder exists. This folder should include subfolder \"architecture_{architecture_num}\" and subfolder \"logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bfd61ffa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Train Loss: 315.2178, Train Acc: 0.42%, Val Loss: 321.2847, Val Acc: 0.39%\n",
      "Epoch [2/100], Train Loss: 235.9790, Train Acc: 2.64%, Val Loss: 279.2842, Val Acc: 2.07%\n",
      "Epoch [3/100], Train Loss: 180.2233, Train Acc: 9.47%, Val Loss: 190.3739, Val Acc: 10.67%\n",
      "Epoch [4/100], Train Loss: 138.3776, Train Acc: 21.36%, Val Loss: 273.5992, Val Acc: 2.64%\n",
      "Epoch [5/100], Train Loss: 108.1334, Train Acc: 33.81%, Val Loss: 150.4012, Val Acc: 25.13%\n",
      "Epoch [6/100], Train Loss: 85.7527, Train Acc: 44.43%, Val Loss: 228.5961, Val Acc: 14.86%\n",
      "Epoch [7/100], Train Loss: 68.4395, Train Acc: 53.09%, Val Loss: 163.5896, Val Acc: 23.11%\n",
      "Epoch [8/100], Train Loss: 55.3748, Train Acc: 60.28%, Val Loss: 138.4757, Val Acc: 35.09%\n",
      "Epoch [9/100], Train Loss: 44.7948, Train Acc: 66.45%, Val Loss: 146.5965, Val Acc: 29.44%\n",
      "Epoch [10/100], Train Loss: 36.9958, Train Acc: 71.38%, Val Loss: 151.1981, Val Acc: 35.87%\n",
      "Epoch [11/100], Train Loss: 29.9925, Train Acc: 76.00%, Val Loss: 166.1921, Val Acc: 26.68%\n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-05.\n",
      "Epoch [12/100], Train Loss: 24.9926, Train Acc: 79.46%, Val Loss: 192.8665, Val Acc: 30.80%\n",
      "Epoch [13/100], Train Loss: 8.4744, Train Acc: 91.99%, Val Loss: 106.8456, Val Acc: 49.15%\n",
      "Epoch [14/100], Train Loss: 6.4418, Train Acc: 93.80%, Val Loss: 98.7032, Val Acc: 51.09%\n",
      "Epoch [15/100], Train Loss: 5.1406, Train Acc: 95.00%, Val Loss: 103.5262, Val Acc: 50.84%\n",
      "Epoch [16/100], Train Loss: 4.2941, Train Acc: 95.87%, Val Loss: 102.6889, Val Acc: 51.80%\n",
      "Epoch [17/100], Train Loss: 3.5693, Train Acc: 96.60%, Val Loss: 143.8885, Val Acc: 41.62%\n",
      "Epoch 00018: reducing learning rate of group 0 to 2.5000e-05.\n",
      "Epoch [18/100], Train Loss: 3.1177, Train Acc: 96.98%, Val Loss: 99.0228, Val Acc: 54.20%\n",
      "Epoch [19/100], Train Loss: 1.0074, Train Acc: 99.23%, Val Loss: 86.8948, Val Acc: 58.94%\n",
      "Epoch [20/100], Train Loss: 0.7814, Train Acc: 99.42%, Val Loss: 87.0734, Val Acc: 59.49%\n",
      "Epoch [21/100], Train Loss: 0.6277, Train Acc: 99.57%, Val Loss: 86.9386, Val Acc: 59.62%\n",
      "Epoch [22/100], Train Loss: 0.5502, Train Acc: 99.64%, Val Loss: 92.8164, Val Acc: 57.74%\n",
      "Epoch 00023: reducing learning rate of group 0 to 1.2500e-05.\n",
      "Epoch [23/100], Train Loss: 0.4962, Train Acc: 99.68%, Val Loss: 98.0096, Val Acc: 56.42%\n",
      "Epoch [24/100], Train Loss: 0.2024, Train Acc: 99.91%, Val Loss: 84.4647, Val Acc: 61.53%\n"
     ]
    }
   ],
   "source": [
    "# Initialize a SummaryWriter\n",
    "writer = SummaryWriter(fr'{path_to_model}\\architecture_{architecture_num}\\logs\\run_{run}')\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_val_loss = np.inf\n",
    "best_loss_epoch = 0\n",
    "best_acc_epoch = 0\n",
    "patience = 0\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "\n",
    "    for data, target in train_dataset:\n",
    "        target = target.type(torch.LongTensor)\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # data = data.permute(0, 3, 1, 2)\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Compute running loss and accuracy\n",
    "        running_loss += loss.item() * data.size(0)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Compute training loss and accuracy\n",
    "    train_loss = running_loss / len(train_dataset)\n",
    "    train_acc = 100.0 * correct / total\n",
    "\n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "    writer.add_scalar('Training Accuracy', train_acc, epoch)\n",
    "\n",
    "    # Evaluate the model on the validation set\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_dataset:\n",
    "            target = target.type(torch.LongTensor)\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # data = data.permute(0, 3, 1, 2)\n",
    "\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            # Compute running loss and accuracy\n",
    "            running_loss += loss.item() * data.size(0)\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Compute validation loss and accuracy\n",
    "    val_loss = running_loss / len(val_dataset)\n",
    "    val_acc = 100.0 * correct / total \n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Write to TensorBoard\n",
    "    writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "    writer.add_scalar('Validation Accuracy', val_acc, epoch)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        patience = 0\n",
    "        best_val_loss = val_loss\n",
    "        best_loss_epoch = epoch\n",
    "        # Save the model's state dictionary\n",
    "        torch.save(model.state_dict(), fr'{path_to_model}\\architecture_{architecture_num}\\best_loss_model.pth')\n",
    "    else:\n",
    "        patience += 1\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_acc_epoch = epoch\n",
    "        # Save the model's state dictionary\n",
    "        torch.save(model.state_dict(), fr'{path_to_model}\\architecture_{architecture_num}\\best_acc_model.pth')\n",
    "\n",
    "    # Print training and validation statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train Los'\n",
    "          f''\n",
    "          f's: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
    "    if train_acc > 99.9:\n",
    "        break\n",
    "\n",
    "torch.save(model.state_dict(), fr'{path_to_model}\\architecture_{architecture_num}\\last_epoch.pth')\n",
    "# Close the SummaryWriter\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3758f",
   "metadata": {},
   "source": [
    "Show the best epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff052de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "best_loss_epoch + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ef0ecee7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc_epoch + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7febc",
   "metadata": {},
   "source": [
    "Quick test of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f7a048",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "_, predicted = torch.max(output.data, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510e6bec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.barplot(x=list(range(968)), y=list(torch.softmax(output.data.cpu(), dim=0)[0].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7073193",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "list(torch.softmax(output.data.cpu(), dim=0)[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b3d04f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.softmax(output.data.cpu(), dim=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39156d33",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(torch.softmax(output.data.cpu(), dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677c87ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(data.cpu()[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e983f56",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937591e",
   "metadata": {},
   "source": [
    "To improve the regularization strength of your model, you can try adjusting the hyperparameters related to regularization techniques, such as L1 or L2 regularization, dropout, or early stopping.\n",
    "\n",
    "L1 and L2 regularization are techniques that add a penalty term to the loss function, which encourages the model to learn simpler weights and reduce overfitting. In PyTorch, you can add L1 or L2 regularization by setting the weight_decay parameter in the optimizer to a nonzero value. For example, you can set weight_decay=0.01 to add L2 regularization to the Adam optimizer:\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "\n",
    "Dropout is a technique that randomly drops out some of the neurons in a layer during training, which helps prevent the model from relying too heavily on specific neurons and reduces overfitting. In your current architecture, you already have two dropout layers with probabilities of 0.25 and 0.5. You can experiment with adjusting the dropout probabilities to see if it improves the model's performance.\n",
    "\n",
    "Early stopping is a technique where you stop training the model once the validation loss stops improving or starts to increase. This helps prevent the model from overfitting to the training data and improves its generalization ability. In PyTorch, you can use the torch.optim.lr_scheduler.ReduceLROnPlateau scheduler to reduce the learning rate when the validation loss stops improving:\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "You can also experiment with adjusting the factor, patience, and verbose parameters to see if it improves the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17e1a630",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Free memory\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ae328",
   "metadata": {},
   "source": [
    "## Instant evaluation logic<a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e99395",
   "metadata": {},
   "source": [
    "[Back to the Table of Contents](#zero-bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3e63974c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# extract each 10th spectrogram from the RHS\n",
    "test_data_set = TensorDataset(*spectrograms[::10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3de0561f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "test_data_set = DataLoader(test_data_set, shuffle=seed, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a604b44b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_dict = torch.load(fr'{path_to_model}\\architecture_{architecture_num}/best_loss_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a41c679",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eee8ff49",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lstm_CNN(\n",
       "  (conv_layers): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU()\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Dropout(p=0.25, inplace=False)\n",
       "  )\n",
       "  (lstm): LSTM(128, 256, num_layers=2, batch_first=True, dropout=0.25, bidirectional=True)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (fc_layers): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=2048, bias=True)\n",
       "    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=2048, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "92eea5c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "running_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_data_set:\n",
    "                target = target.type(torch.LongTensor)\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                # data = data.permute(0, 3, 1, 2)\n",
    "\n",
    "                # Forward pass\n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "                # Compute running loss and accuracy\n",
    "                running_loss += loss.item() * data.size(0)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "\n",
    "    # Compute validation loss and accuracy\n",
    "    val_loss = running_loss / len(test_data_set)\n",
    "    val_acc = 100.0 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b78e067c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation - Val Loss: 0.0149, Val Acc: 100.00%\n"
     ]
    }
   ],
   "source": [
    "print(f'Evaluation - Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedfb1d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
